<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>No Evidence for Epistemic Entropy Collapse – A Null Result | Course Correct Labs</title>
  <meta name="description" content="This study tested whether internal activation entropy declines during long-form generation in small open models. The result is a stable null." />
  <link rel="canonical" href="https://coursecorrectlabs.com/studies/entropy-collapse-null" />
  <link rel="icon" type="image/png" href="../assets/images/favicon.png" />
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "No Evidence for Epistemic Entropy Collapse in Small Open Language Models",
    "author": {"@type": "Person", "name": "Bentley DeVilling"},
    "publisher": {"@type": "Organization", "name": "Course Correct Labs", "url": "https://coursecorrectlabs.com"},
    "url": "https://coursecorrectlabs.com/studies/entropy-collapse-null",
    "description": "This study tested whether internal activation entropy declines during long-form generation in small open models. The result is a stable null."
  }
  </script>
  <style>
    :root{--bg:#000;--fg:#fff;--muted:rgba(255,255,255,.75);--blue:#3b82f6;--teal:#14b8a6;--orange:#FF6F00;--card:rgba(255,255,255,.06);--border:rgba(255,255,255,.12)}
    *{box-sizing:border-box}html{scroll-behavior:smooth}body{margin:0;font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;color:var(--fg);background:#000}a{text-decoration:none;color:var(--fg)}.container{max-width:1280px;margin:0 auto;padding:0 1.25rem}header{position:sticky;top:0;z-index:50;backdrop-filter:saturate(180%) blur(6px);background:rgba(0,0,0,.45)}.nav{height:64px;display:flex;align-items:center;justify-content:space-between}.brand{display:flex;align-items:center;gap:.65rem;font-weight:600;letter-spacing:.2px}.logo{height:48px;width:auto;display:block}.menu{display:none;gap:1rem;font-size:.95rem}.menu a{opacity:.8}.menu a.active{color:var(--blue);opacity:1}.hamburger{display:inline-flex;background:transparent;border:0;color:var(--fg);padding:.5rem;border-radius:.5rem}@media (min-width:768px){.menu{display:flex}.hamburger{display:none}}.mobile{display:none;border-top:1px solid var(--border);background:rgba(0,0,0,.7)}.mobile a{display:block;padding:.75rem 1rem;border-bottom:1px solid var(--border);opacity:.85}.hero{position:relative;padding:60px 0 40px;background:#000}h1,h2,h3{font-family:'Avenir Next',-apple-system,system-ui,sans-serif}.title-container{max-width:1100px;margin:0 auto}h1{font-size:28px;margin:0 0 1rem;line-height:1.2;text-align:center}section{padding:80px 0}.paper-hero{max-width:800px;margin:0 auto}.status-text{font-style:italic;font-size:.9rem;color:var(--muted);text-align:center;margin:0 0 2rem}.paper-img{width:100%;max-width:600px;aspect-ratio:1/1;border-radius:12px;background:#111318;object-fit:cover;display:block;margin:2rem auto}.archive-note{font-style:italic;color:var(--muted);text-align:center;max-width:700px;margin:0 auto 3rem;font-size:1rem;line-height:1.7}.section-heading{text-transform:uppercase;font-size:1.02rem;font-weight:600;letter-spacing:.15em;color:var(--orange);margin:3rem 0 1rem;text-align:center}.content p{color:var(--muted);font-size:1rem;line-height:1.7;margin:1rem 0}.content ul{color:var(--muted);font-size:1rem;line-height:1.7;margin:1rem 0;padding-left:1.5rem}.content ul li{margin:.5rem 0}.links-text{color:var(--muted);margin:0;font-size:.95rem}footer{padding:48px 0;color:var(--muted);font-size:.95rem;text-align:center;background:#000}.footer-logo{height:64px;width:auto;margin:0 auto 24px;display:block;opacity:.8}.footer-links{display:flex;align-items:center;justify-content:center;gap:16px;flex-wrap:wrap;margin-bottom:16px}.footer-links a{display:inline-flex;align-items:center;justify-content:center;width:32px;height:32px;border-radius:50%;background:rgba(255,255,255,.08);transition:background .2s}.footer-links a:hover{background:rgba(255,255,255,.15)}.footer-links a svg{width:16px;height:16px;fill:currentColor}
  </style>
</head>
<body>
  <header>
    <div class="container nav">
      <a href="https://coursecorrectlabs.com" class="brand"><img src="../assets/images/logo.png" alt="Course Correct Labs" class="logo" /></a>
      <nav class="menu" id="menu">
        <a href="/research">Research</a>
        <a href="/studies" class="active">Studies</a>
        <a href="/evaluations">Evaluations</a>
        <a href="/partner-with-us">Partner With Us</a>
        <a href="/about">About</a>
        <a href="/contact">Contact</a>
      </nav>
      <button class="hamburger" id="hambtn">☰</button>
    </div>
    <div class="mobile" id="mobile">
      <div class="container">
        <a href="/research">Research</a><a href="/studies">Studies</a><a href="/evaluations">Evaluations</a><a href="/partner-with-us">Partner With Us</a><a href="/about">About</a><a href="/contact">Contact</a>
      </div>
    </div>
  </header>
  <main>
    <section class="hero">
      <div class="container paper-hero">
        <div class="title-container">
          <h1>No Evidence for Epistemic Entropy Collapse in Small Open Language Models</h1>
        </div>
        <p class="status-text">Archived null finding</p>
        <img class="paper-img" src="../assets/images/eec-studies-desktop.png" alt="Epistemic Entropy Collapse visualization" loading="lazy" />

        <p class="archive-note">This study tested whether internal activation entropy declines during long-form generation in small open models. The result is a stable null: no reliable collapse signal, no predictive relationship between early-window entropy and end-of-sequence failure, and no coupling between internal metrics and semantic drift.</p>

        <div class="content">
          <div class="section-heading">Summary</div>
          <p>This study tests a specific claim: that internal activation entropy in language models declines during long-form generation, causing hidden states to compress into a lower-dimensional space. If the hypothesis were true, internal structure would shrink as the model generates text, and early signs of collapse would forecast failures later in the sequence.</p>
          <p>The experiment analyzes two open models, Phi-2 and Mistral-7B, across 346 long outputs. For each generation, the study extracts hidden states at the final transformer layer, computes effective rank, participation ratio, and activation variance over sliding windows, and aligns those metrics with external measures like semantic drift, trigram novelty, and QA success.</p>
          <p>The result is a clean null. Internal dimensionality stays flat. Only about ten percent of sequences show even mild negative slopes, and those are symmetric with positive slopes. Early entropy does not predict failure. There is no correlation between internal dynamics and external drift. The study establishes that, for small open models producing about eight hundred tokens, internal activation structure remains stable under normal conditions.</p>
          <p>The result does not prove collapse is impossible. It sets a boundary. If collapse arises, it is likely scale specific, length specific, or prompt specific. The study provides a reproducible pipeline for testing those regimes and a benchmark null for future claims.</p>

          <div class="section-heading">Abstract</div>
          <p>We tested whether internal activation entropy systematically declines during long-form generation in open-weight language models, a proposed signature of epistemic entropy collapse. Using a reproducible mechanistic pipeline, we measured variance, effective rank, and participation ratio of hidden states in Phi-2 and Mistral-7B across 346 prompts producing eight hundred token outputs. We found no consistent decline in internal metrics and no predictive relationship between early-window entropy and end-of-sequence failure (ROC AUC about 0.46). Mean ECI was essentially zero with only about ten percent of prompts showing small negative values. The relationship between entropy decline and semantic drift was near zero at both sequence and window scales. These null results indicate that, for small open transformers and typical generation lengths, internal representations remain dynamically stable. We release the code and outputs as a reproducible null benchmark for long-context activation dynamics and as a calibration point for future claims about collapse at larger scales or under specialized prompting. Total compute cost was about eighteen dollars on consumer hardware.</p>

          <div class="section-heading">Why It Matters</div>
          <p>Claims about entropy collapse come with strong implications. If internal representations compress by default, long-form reasoning would be fundamentally unstable. Systems would require constant grounding, and failures would accumulate as sequences grow. If collapse does not happen under normal settings, researchers can focus on other causes of drift and error.</p>
          <p>This study shows that collapse is not a default behavior in small open models. Internal geometry stays steady. Failures arise, but not because the hidden space is compressing. This clarifies where collapse is not happening and narrows the range of conditions where it might. It also demonstrates that mechanistic interpretability can be executed at low cost and provides a blueprint for scaling the experiment to larger models, longer contexts, and recursive prompting regimes.</p>

          <div class="section-heading">Key Findings</div>
          <ul>
            <li>No systematic decline in effective rank, participation ratio, or variance</li>
            <li>About ten percent of sequences show small negative slopes, symmetric with positive slopes</li>
            <li>Early window entropy does not predict end-of-sequence failures</li>
            <li>Correlation between entropy dynamics and semantic drift is near zero</li>
            <li>Internal dimensionality stays stable across eight hundred token outputs</li>
            <li>Failures are not caused by collapsing internal geometry</li>
            <li>Null result bounds the space where collapse claims should be tested next</li>
            <li>The pipeline runs on consumer hardware and is fully reproducible</li>
          </ul>

          <div style="text-align:center;margin-top:3rem">
            <a href="https://github.com/Course-Correct-Labs/entropy-collapse-null" target="_blank" rel="noopener noreferrer" class="btn" style="display:inline-flex;align-items:center;gap:.5rem;padding:.7rem 1rem;border-radius:.6rem;border:1px solid var(--border);background:#111318;color:var(--fg);font-size:.9rem">GitHub →</a>
          </div>
        </div>
      </div>
    </section>
  </main>
  <footer>
    <div class="container">
      <img src="../assets/images/footer-logo.png" alt="Course Correct Labs" class="footer-logo" />
      <div class="footer-links">
        <a href="mailto:hello@coursecorrectlabs.com" aria-label="Email"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 4H4c-1.1 0-1.99.9-1.99 2L2 18c0 1.1.9 2 2 2h16c1.1 0 2-.9 2-2V6c0-1.1-.9-2-2-2zm0 4l-8 5-8-5V6l8 5 8-5v2z"/></svg></a>
        <a href="https://github.com/Course-Correct-Labs" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg></a>
        <a href="https://x.com/coursecorrect" target="_blank" rel="noopener noreferrer" aria-label="X"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/></svg></a>
        <a href="https://linkedin.com/company/course-correct-labs" target="_blank" rel="noopener noreferrer" aria-label="LinkedIn"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/></svg></a>
      </div>
      <div>© <span id="yr"></span> Course Correct Labs</div>
    </div>
  </footer>
  <script>
    document.getElementById('yr').textContent=new Date().getFullYear();const hambtn=document.getElementById('hambtn');const mobile=document.getElementById('mobile');hambtn.addEventListener('click',()=>{const open=mobile.style.display==='block';mobile.style.display=open?'none':'block';hambtn.setAttribute('aria-expanded',open?'false':'true');});mobile.querySelectorAll('a').forEach(a=>a.addEventListener('click',()=>{mobile.style.display='none';hambtn.setAttribute('aria-expanded','false');}));
  </script>
</body>
</html>
